% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
% 1) Include xcolor with the [table] option so cell coloring works.
\usepackage[table]{xcolor}

% 2) Include soul so \hl and \sethlcolor work.
\usepackage{soul}

% 3) Define any custom colors you actually use in the table.
\definecolor{pastelyellow}{rgb}{1.0, 1.0, 0.8}
\definecolor{lightgray}{gray}{0.92}
\definecolor{teagreen}{rgb}{0.82, 0.94, 0.75}
\definecolor{pinksecondbest}{rgb}{1.0, 0.82, 0.86}
\definecolor{pastelviolet}{rgb}{0.87, 0.81, 0.96}
\definecolor{predcolor}{rgb}{1.0, 0.86, 0.85}
% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Audio Project}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Eltar Mazoz \\
  Tel Aviv University \\
  \texttt{email@mail.tau.ac.il} \\\And
  Omer Talmi \\
  Tel Aviv University \\
  \texttt{omertalmi@mail.tau.ac.il} \\}

\begin{document}
\maketitle
\begin{abstract}
This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style file for ACL 2023.
The document itself conforms to its own specifications, and is, therefore, an example of what your manuscript should look like.
These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\section{Introduction}

These instructions are for authors submitting papers to ACL 2023 using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} as well as guidelines set forth in the ACL 2023 call for papers.\footnote{\url{https://2023.aclweb.org/calls/main_conference/}} This document contains additional instructions for the \LaTeX{} style files.
The templates include the \LaTeX{} source of this document (\texttt{acl2023.tex}),
the \LaTeX{} style file used to format it (\texttt{acl2023.sty}),
an ACL bibliography style (\texttt{acl\_natbib.bst}),
an example bibliography (\texttt{custom.bib}),
and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

\section{Related Work}
One of the big breakthroughs in deep learning for vision tasks was the introduction of the Vision Transformer (ViT), which builds upon the transformer architecture (Vaswani et al., 2017). Unlike convolutional neural networks (CNNs), ViT processes images as sequences of patches, leveraging self-attention mechanisms to model long-range dependencies across an image (Dosovitskiy et al., 2020). This shift in paradigm has led to competitive or superior performance compared to CNNs, especially when trained on large datasets. ViT’s success has also extended beyond vision, inspiring models such as the Audio Spectrogram Transformer (AST) for audio classification, demonstrating the versatility of transformer-based architectures in various modalities (Wu et al., 2023) (arxiv.org).


\section{Method}
\subsection{Overview of Parameter-efficient Transfer Learning Methods}

We now introduce the PETL techniques we used in our experiments: LoRA, Pref-T 24, Linear probing, adapter tuning, and full fine-tuning.
We also introduce our novel method to use freq-LoRA for finetuning AST.

\subsubsection{Full Fine-Tuning}
Full fine-tuning (FFT) adapts the entire pre-trained model to each downstream task. This approach ensures optimal performance by adjusting all model parameters, but it requires significant computational resources and storage. Fine-tuning the full model is often impractical for resource-constrained environments, particularly in multi-task learning scenarios where maintaining separate models for each task becomes infeasible.

\subsubsection{Pref-T 24 (Prefix-Tuning)}
Prefix-tuning inserts a set of $p$ learnable continuous embeddings of dimension $d$ (i.e., prompts) into the keys and values of the MHSA block at every layer. Pref-T 24 follows the deep prompt-tuning (DPT) paradigm, where prompts are uniformly prepended to each transformer layer instead of just the first layer, as in shallow prompt-tuning (SPT). By strategically placing the prefix embeddings throughout the network, Pref-T 24 enhances the model’s ability to generalize across tasks while maintaining parameter efficiency.

\subsubsection{Linear Probing}
Linear probing is a lightweight fine-tuning method where only the classification head (i.e., the final linear layer) of the pre-trained model is updated, while the rest of the model remains frozen. This approach significantly reduces computational cost and prevents catastrophic forgetting by preserving the original model’s learned representations.

Given an input sequence $X_{\text{in}}$, the output of the frozen transformer layers is represented as:

\begin{equation}
X_{\text{out}} = \text{Transformer}(X_{\text{in}})
\end{equation}

A task-specific linear classification head $W_{\text{lin}} \in \mathbb{R}^{d \times C}$, where $C$ is the number of output classes, is then applied to obtain the final predictions:

\begin{equation}
\hat{y} = X_{\text{out}} W_{\text{lin}}
\end{equation}

Although linear probing is highly efficient and requires only a small number of trainable parameters, its performance is often inferior to other PETL methods such as LoRA or adapter-based tuning, especially for tasks requiring deeper model adaptation. However, it remains a strong baseline for transfer learning scenarios where computational resources are limited.

\subsubsection{LoRA (Low-Rank Adaptation)}
LoRA introduces trainable low-rank matrices into transformer layers to approximate the weight updates. For a pre-trained weight matrix $W \in \mathbb{R}^{d \times d_k}$, LoRA represents its update with a low-rank decomposition $W + \Delta W = W + AB$, where $A \in \mathbb{R}^{d \times r}$, $B \in \mathbb{R}^{r \times d}$, and $r \ll d$. LoRA typically applies this update to the query and value projection matrices, $W_q$ and $W_v$, in the Multi-Head Self-Attention (MHSA) sub-layer. The computation of query and value matrices follows:

\begin{equation}
Q/V = X_{\text{in}} W_{q/v} + s \cdot X_{\text{in}} A_{q/v} B_{q/v}
\end{equation}

where $s$ is a tunable scalar hyperparameter. This method allows efficient adaptation of the pre-trained model by introducing a minimal number of trainable parameters, thereby reducing storage requirements.

\subsubsection{Conformer Adapter: Pfeiffer Configuration}
Adapters are small, trainable modules inserted into transformer layers. They employ a bottleneck structure to minimize additional parameters while still enabling task-specific adaptation. Specifically, the hidden representation of dimension $d$ is first down-projected to a smaller dimension $r$, then passed through a non-linear transformation, and finally up-projected back to $d$. Formally, for a transformer layer output $X_{\text{FF}}$ from the feed-forward (FF) block and a residual connection $X̂$, the Pfeiffer configuration places an adapter \emph{after} the FF block:
\begin{equation}
X_{\text{out}} \;=\; X̂ \;+\; X_{\text{FF}} \;+\; f\bigl(X̂\,W_{\text{down}}\bigr)\,W_{\text{up}},
\end{equation}
where $W_{\text{down}} \in \mathbb{R}^{d \times r}$ and $W_{\text{up}} \in \mathbb{R}^{r \times d}$, and $f(\cdot)$ is a non-linear activation function (e.g., ReLU). 

\paragraph{Motivation for Integrating Convolution.}
While a purely linear bottleneck adapter can be effective, it may struggle to capture local and contextual information in speech or audio tasks. Hence, we integrate a lightweight convolution module inspired by Conformer, retaining the Pfeiffer placement scheme to keep the number of additional parameters low.

\subsubsection{Proposed Conformer Adapter}
To better model local dependencies while preserving the bottleneck design, we replace the simple down-up linear layers with a \textbf{depthwise convolution} module from the Conformer architecture. This design greatly enhances performance on speech and audio tasks:

\begin{enumerate}
    \item \textbf{Pointwise Down Convolution}: The input $X̂$ is projected to a hidden dimension of $2r$. 
    \item \textbf{Gated Linear Unit (GLU)}: This reduces the projected feature size to $r$.
    \item \textbf{Depthwise Convolution}: A depthwise convolution of kernel size $k$ is applied, followed by Batch Normalization and a Swish activation function. This efficiently captures local patterns with minimal extra parameters.
    \item \textbf{Pointwise Up Convolution}: Finally, we project the hidden representation back to dimension $d$.
\end{enumerate}

Placed after the FF block (as in Pfeiffer), the Conformer Adapter can significantly improve performance on challenging audio or speech tasks. By leveraging depthwise convolutions in a bottleneck structure, our proposed approach offers notable gains over purely linear adapters, yet still requires only a fraction of the parameters used in full fine-tuning.


\begingroup
\setlength{\tabcolsep}{3.3pt}
\newcolumntype{K}{!{\color{white}\ }c}

\begin{table}[t]
\centering
\caption{Performance evaluations of the PETL methods on ESC and US8K for AST. 
Best and second-best performances for each dataset are coloured in 
\sethlcolor{teagreen}\hl{\textbf{Green}} and 
\sethlcolor{pinksecondbest}\hl{Red}, respectively.}
\label{tab:main}
\begin{tabular}{lKKK}
\toprule
\textbf{Method} & \textbf{Par} & \cellcolor{pastelyellow}\textbf{ESC} & \cellcolor{lightgray}\textbf{US8K}\\
\midrule
FFT &  85M & 87.50 & 83.08\\
Linear     & 9/40K   & 74.64      & 76.67\\
\hline \addlinespace
Pref-T 24  & 221K    & 82.75      & 81.11\\ 
LoRA       & 221K    & 85.80      & \cellcolor{pinksecondbest}84.12\\
\hline
\rowcolor{pastelviolet}
\multicolumn{4}{l}{\textbf{Conformer Adapter}}\\
\hline \addlinespace
Pfeiffer   & 271K    & \cellcolor{pinksecondbest}87.30 & \cellcolor{teagreen}\textbf{79.25}\\
\bottomrule
\end{tabular}
\end{table}
\endgroup


\section{Experiments}
\subsection{ Implementation Details}
\textbf{Datasets}. We evaluate the PETL methods on two audio downstream classification tasks.  \textbf{Audio classification}: we use the ESC-50 and UrbanSound8K (US8K) datasets. ESC-50 (ESC) \cite{piczak2015esc} consists of $2,000$ 5-second-long environmental audio recordings of $50$ classes. US8K \cite{salamon2014dataset} includes $8,732$ labeled sound excerpts of urban sounds from $10$ classes.

\textbf{Baselines}. We include the \textit{full fine-tuning} method (FFT), which finetunes the full pre-trained AST model; and \textit{linear probing}, which only fine-tunes the classification head. We then study various PETL methods: shallow prompt-tuning (SPT), deep prompt-tuning (DPT), prefix-tuning (Pref-T), and BitFit \cite{zaken2021bitfit}, which is a common baseline that fine-tunes only the bias terms of the pre-trained
backbone. SPT adds all the $300$ prompts to the input of the first transformer layer, whereas DPT adds $25$ prompts to each transformer layer. We then include LoRA and bottleneck and conformer (ours) adapters. The dimension of the intermediate space for adapters and LoRA is $r = d/\text{RR}$, where $d = 768$ is the hidden dimension of the AST model and RR is the reduction rate. Unless otherwise stated, $r$ is set to $12$, $8$, and $6$ for bottleneck adapter, conformer, and LoRA, respectively. In this way, the resulting number of parameters is roughly the same. For LoRA, following \cite{hu2021lora}, the scaling factor is set to $s = \alpha/\text{RR}$, where $\alpha$ = 16 leads to the best results (i.e., $s = 8$). We also note that each adapter module is added in parallel to only the MHSA layer (Pfeiffer) or both the MHSA and FF layers (Houslby). For this reason, Houlsby adapters require twice as many parameters as Pfeiffer. Inserting the adapters sequentially leads to slightly worse results, yet we do not include these results for lack of space. Finally, for the speech tasks we set the kernel size of the depthwise convolution layer to 31, which is the original value proposed in \cite{conformer}, while for audio tasks we found that $k = 8$ gives the best results (we refer the reader to Section \ref{sec:ablations} for a detailed analysis). 

\textbf{Computational Setup and Challenges.} Initially, we trained our models using the university's SLURM-based cluster. However, due to memory constraints, we had to significantly reduce the batch size to fit the model into the available resources. This led to suboptimal convergence and degraded performance. To overcome these limitations, we migrated to Google Colab with an A100 GPU, allowing for larger batch sizes and more stable training. Nonetheless, the performance remained unsatisfactory until we carefully tuned the Reduction Ratio (RR) of the adapters. We observed that increasing RR from 64 to 96 significantly improved accuracy, suggesting that higher compression levels helped regularize the model and reduce overfitting. This highlights the importance of proper RR selection when applying PETL methods to AST-based models.
\subsection{Citations}



Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,augenstein-etal-2016-stance,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}
You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}
Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Limitations}
ACL 2023 requires all submissions to have a section titled ``Limitations'', for discussing the limitations of the paper as a complement to the discussion of strengths in the main text. This section should occur after the conclusion, but before the references. It will not count towards the page limit.
The discussion of limitations is mandatory. Papers without a limitation section will be desk-rejected without review.

While we are open to different types of limitations, just mentioning that a set of results have been shown for English only probably does not reflect what we expect. 
Mentioning that the method works mostly for languages with limited morphology, like English, is a much better alternative.
In addition, limitations such as low scalability to long text, the requirement of large GPU resources, or other things that inspire crucial further investigation are welcome.

\section*{Ethics Statement}
Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

\section*{Acknowledgements}
This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
ACL 2012 by Maggie Li and Michael White, 
ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
ACL 2002 by Eugene Charniak and Dekang Lin, 
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}
