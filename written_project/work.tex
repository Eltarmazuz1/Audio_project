oject% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Audio Project}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Eltar Mazoz \\
  Affiliation / Address line 1 \\
  \texttt{email@domain} \\\And
  Omer Talmi \\
  Affiliation / Address line 1 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style file for ACL 2023.
The document itself conforms to its own specifications, and is, therefore, an example of what your manuscript should look like.
These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\section{Introduction}

These instructions are for authors submitting papers to ACL 2023 using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} as well as guidelines set forth in the ACL 2023 call for papers.\footnote{\url{https://2023.aclweb.org/calls/main_conference/}} This document contains additional instructions for the \LaTeX{} style files.
The templates include the \LaTeX{} source of this document (\texttt{acl2023.tex}),
the \LaTeX{} style file used to format it (\texttt{acl2023.sty}),
an ACL bibliography style (\texttt{acl\_natbib.bst}),
an example bibliography (\texttt{custom.bib}),
and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

\section{Related Work}
One of the big breakthroughs in deep learning for vision tasks was the introduction of the Vision Transformer (ViT), which builds upon the transformer architecture (Vaswani et al., 2017). Unlike convolutional neural networks (CNNs), ViT processes images as sequences of patches, leveraging self-attention mechanisms to model long-range dependencies across an image (Dosovitskiy et al., 2020). This shift in paradigm has led to competitive or superior performance compared to CNNs, especially when trained on large datasets. ViT’s success has also extended beyond vision, inspiring models such as the Audio Spectrogram Transformer (AST) for audio classification, demonstrating the versatility of transformer-based architectures in various modalities (Wu et al., 2023) (arxiv.org).


\section{Method}
\subsection{Overview of Parameter-efficient Transfer Learning Methods}

We now introduce the PETL techniques we used in our experiments: LoRA, Pref-T 24, Linear probing, adapter tuning, and full fine-tuning.

\subsubsection{Full Fine-Tuning}
Full fine-tuning (FFT) adapts the entire pre-trained model to each downstream task. This approach ensures optimal performance by adjusting all model parameters, but it requires significant computational resources and storage. Fine-tuning the full model is often impractical for resource-constrained environments, particularly in multi-task learning scenarios where maintaining separate models for each task becomes infeasible.

\subsubsection{Pref-T 24 (Prefix-Tuning)}
Prefix-tuning inserts a set of $p$ learnable continuous embeddings of dimension $d$ (i.e., prompts) into the keys and values of the MHSA block at every layer. Pref-T 24 follows the deep prompt-tuning (DPT) paradigm, where prompts are uniformly prepended to each transformer layer instead of just the first layer, as in shallow prompt-tuning (SPT). By strategically placing the prefix embeddings throughout the network, Pref-T 24 enhances the model’s ability to generalize across tasks while maintaining parameter efficiency.

\subsubsection{Linear Probing}
Linear probing is a lightweight fine-tuning method where only the classification head (i.e., the final linear layer) of the pre-trained model is updated, while the rest of the model remains frozen. This approach significantly reduces computational cost and prevents catastrophic forgetting by preserving the original model’s learned representations.

Given an input sequence $X_{\text{in}}$, the output of the frozen transformer layers is represented as:

\begin{equation}
X_{\text{out}} = \text{Transformer}(X_{\text{in}})
\end{equation}

A task-specific linear classification head $W_{\text{lin}} \in \mathbb{R}^{d \times C}$, where $C$ is the number of output classes, is then applied to obtain the final predictions:

\begin{equation}
\hat{y} = X_{\text{out}} W_{\text{lin}}
\end{equation}

Although linear probing is highly efficient and requires only a small number of trainable parameters, its performance is often inferior to other PETL methods such as LoRA or adapter-based tuning, especially for tasks requiring deeper model adaptation. However, it remains a strong baseline for transfer learning scenarios where computational resources are limited.

\subsubsection{LoRA (Low-Rank Adaptation)}
LoRA introduces trainable low-rank matrices into transformer layers to approximate the weight updates. For a pre-trained weight matrix $W \in \mathbb{R}^{d \times d_k}$, LoRA represents its update with a low-rank decomposition $W + \Delta W = W + AB$, where $A \in \mathbb{R}^{d \times r}$, $B \in \mathbb{R}^{r \times d}$, and $r \ll d$. LoRA typically applies this update to the query and value projection matrices, $W_q$ and $W_v$, in the Multi-Head Self-Attention (MHSA) sub-layer. The computation of query and value matrices follows:

\begin{equation}
Q/V = X_{\text{in}} W_{q/v} + s \cdot X_{\text{in}} A_{q/v} B_{q/v}
\end{equation}

where $s$ is a tunable scalar hyperparameter. This method allows efficient adaptation of the pre-trained model by introducing a minimal number of trainable parameters, thereby reducing storage requirements.

\subsubsection{Bottleneck Adapter: Pfeiffer and Houlsby Configurations}
Adapters are small, trainable modules added to transformer layers, following a bottleneck structure. The input sequence of hidden dimension $d$ is first down-projected into a low-dimensional space using $W_{\text{down}} \in \mathbb{R}^{d \times r}$, followed by a non-linear activation function $f(\cdot)$, and then up-projected back to the original dimension $d$ using $W_{\text{up}} \in \mathbb{R}^{r \times d}$. This design minimizes additional parameters while enabling task-specific adaptation.

Adapters can be placed in different configurations:
\begin{itemize}
    \item \textbf{Pfeiffer Configuration}: The adapter is inserted only after the feedforward (FF) block.
    \item \textbf{Houlsby Configuration}: The adapter is placed after both the MHSA and FF blocks, effectively doubling the number of trainable parameters compared to the Pfeiffer configuration.
\end{itemize}

For the Pfeiffer adapter, the output is computed as:

\begin{equation}
X_{\text{out}} = X̂ + X_{\text{FF}} + f(X̂ W_{\text{down}}) W_{\text{up}}
\end{equation}

where $X_{\text{FF}}$ is the output of the FFN layer.

\subsubsection{Conformer Adapter (Proposed)}
The standard bottleneck adapter struggles with speech tasks due to its linear nature. To address this, we propose a \textbf{Conformer Adapter}, which integrates the depthwise convolution module from the Conformer architecture. This module captures local spatial correlations and reduces the number of parameters compared to traditional CNNs.

The Conformer Adapter follows this pipeline:
\begin{enumerate}
    \item \textbf{Pointwise Down Convolution}: Projects the input to a hidden dimension of $2r$.
    \item \textbf{Gated Linear Unit (GLU)}: Reduces the hidden dimension to $r$.
    \item \textbf{Depthwise Convolution}: Applies depthwise convolution with a kernel size $k$, followed by Batch Normalization and Swish activation.
    \item \textbf{Pointwise Up Convolution}: Projects the hidden representation back to dimension $d$.
\end{enumerate}

By leveraging depthwise convolutions, the Conformer Adapter significantly improves performance on speech tasks while using only 0.29\% of the parameters compared to full fine-tuning.



\section{Document Body}

\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

See Table~\ref{tab:accents} for an example of a table and its caption.
\textbf{Do not override the default caption sizes.}

\subsection{Hyperlinks}

Users of older versions of \LaTeX{} may encounter the following error during compilation: 
\begin{quote}
\tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
\end{quote}
This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

\subsection{Citations}



Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,augenstein-etal-2016-stance,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}
You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}
Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Limitations}
ACL 2023 requires all submissions to have a section titled ``Limitations'', for discussing the limitations of the paper as a complement to the discussion of strengths in the main text. This section should occur after the conclusion, but before the references. It will not count towards the page limit.
The discussion of limitations is mandatory. Papers without a limitation section will be desk-rejected without review.

While we are open to different types of limitations, just mentioning that a set of results have been shown for English only probably does not reflect what we expect. 
Mentioning that the method works mostly for languages with limited morphology, like English, is a much better alternative.
In addition, limitations such as low scalability to long text, the requirement of large GPU resources, or other things that inspire crucial further investigation are welcome.

\section*{Ethics Statement}
Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

\section*{Acknowledgements}
This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
ACL 2012 by Maggie Li and Michael White, 
ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
ACL 2002 by Eugene Charniak and Dekang Lin, 
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}
