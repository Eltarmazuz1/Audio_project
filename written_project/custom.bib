@inproceedings{piczak2015esc,
  title={ESC: Dataset for environmental sound classification},
  author={Piczak, K.},
  booktitle={ACM Multimedia},
  pages={1015--1018},
  year={2015}
}

@inproceedings{salamon2014dataset,
  title={A dataset and taxonomy for urban sound research},
  author={Salamon, J. and others},
  booktitle={ACM Multimedia},
  year={2014}
}

@article{dosovitskiy2020image,
  author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  journal      = {arXiv preprint arXiv:2010.11929},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11929},
  eprint       = {2010.11929},
  archivePrefix= {arXiv}
}

@inproceedings{gong2021ast,
  title={Ast: Audio spectrogram transformer},
  author={Gong, Y. and Chung, Y. and Glass, J.},
  booktitle={Interspeech},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, X. and Liang, P.},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@inproceedings{he2021towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, J. and others},
  booktitle={ICLR},
  year={2022}
}


@misc{ly2024enhancing,
  title        = {Enhancing Parameter-Efficient Fine-Tuning of Vision Transformers through Frequency-Based Adaptation},
  author       = {Son Thai Ly and Hien V. Nguyen},
  year         = {2024},
  archivePrefix= {arXiv},
  eprint       = {2411.19297},
  primaryClass = {cs.CV},
  url          = {https://arxiv.org/abs/2411.19297}
}


@inproceedings{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, E. and others},
  booktitle={ICLR},
  year={2022}
}

@misc{kumar2022finetuning,
  title        = {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author       = {Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},
  year         = {2022},
  archivePrefix= {arXiv},
  eprint       = {2202.10054},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2202.10054}
}
 

@article{xu2023parameter,
  title={Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
  author={Xu, L. and others},
  journal={arXiv preprint arXiv:2312.12148},
  year={2023}
}

@inproceedings{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and ≈Åukasz Kaiser and Illia Polosukhin},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {30},
  pages     = {5998--6008},
  year      = {2017},
  url       = {https://arxiv.org/abs/1706.03762}
}